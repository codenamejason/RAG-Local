{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Implementation with Anthropic Claude and OpenAI\n",
        "\n",
        "This notebook demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline using:\n",
        "- **OpenAI** for semantic embeddings (Ada-002)\n",
        "- **Anthropic Claude** for generation (Haiku/Sonnet/Opus)\n",
        "- **ChromaDB** for local vector storage\n",
        "\n",
        "A production-ready implementation for building AI-powered search and Q&A systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup - Install required packages\n",
        "# Using UV for fast package management (10-100x faster than pip)\n",
        "\n",
        "# Run this in terminal:\n",
        "# .\\setup_uv.ps1  # Windows\n",
        "# ./setup_uv.sh    # Linux/Mac\n",
        "\n",
        "# Or install with uv:\n",
        "# uv pip install anthropic openai chromadb python-dotenv pandas numpy scikit-learn tenacity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path so we can import our modules\n",
        "sys.path.append(str(Path.cwd()))\n",
        "\n",
        "# Import required libraries\n",
        "import anthropic\n",
        "import openai\n",
        "import chromadb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Import our RAG modules\n",
        "from src.rag_pipeline import RAGPipeline\n",
        "from src.embeddings import OpenAIEmbeddings\n",
        "from src.vector_store import VectorStore\n",
        "from src.chunking import TextChunker, MarkdownChunker\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"Anthropic version: {anthropic.__version__}\")\n",
        "print(f\"OpenAI version: {openai.__version__}\")\n",
        "print(f\"Current directory: {Path.cwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Configure API Keys\n",
        "\n",
        "Make sure you have set the following environment variables in your `.env` file:\n",
        "- `ANTHROPIC_API_KEY` - For Claude text generation\n",
        "- `OPENAI_API_KEY` - For embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if API keys are configured\n",
        "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not anthropic_key:\n",
        "    print(\"‚ùå ANTHROPIC_API_KEY not found in environment variables\")\n",
        "    print(\"   Get your key at: https://console.anthropic.com/\")\n",
        "else:\n",
        "    print(f\"‚úÖ Anthropic API key configured: {anthropic_key[:10]}...\")\n",
        "\n",
        "if not openai_key:\n",
        "    print(\"‚ùå OPENAI_API_KEY not found in environment variables\")\n",
        "    print(\"   Get your key at: https://platform.openai.com/\")\n",
        "else:\n",
        "    print(f\"‚úÖ OpenAI API key configured: {openai_key[:10]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Initialize RAG Pipeline\n",
        "\n",
        "We'll create a RAG pipeline with:\n",
        "- Claude 3 Haiku for fast, cost-effective generation\n",
        "- OpenAI Ada-002 embeddings for semantic search (1536 dimensions)\n",
        "- ChromaDB for local vector storage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the RAG pipeline\n",
        "rag = RAGPipeline(\n",
        "    model=\"claude-3-haiku-20240307\",  # Fast and cost-effective\n",
        "    collection_name=\"demo_collection\",\n",
        "    chunk_size=512,  # Characters per chunk\n",
        "    chunk_overlap=50  # Overlap between chunks\n",
        ")\n",
        "\n",
        "# Clear any existing data\n",
        "rag.clear_knowledge_base()\n",
        "\n",
        "print(\"‚úÖ RAG pipeline initialized!\")\n",
        "print(f\"üìä Stats: {rag.get_stats()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Add Documents to Knowledge Base\n",
        "\n",
        "Let's add some sample documents about AI and machine learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample documents\n",
        "documents = [\n",
        "    \"\"\"\n",
        "    # Large Language Models (LLMs)\n",
        "    \n",
        "    Large Language Models are neural networks trained on vast amounts of text data.\n",
        "    They can understand and generate human-like text across many domains.\n",
        "    \n",
        "    ## Key Capabilities\n",
        "    - Text generation and completion\n",
        "    - Question answering\n",
        "    - Summarization\n",
        "    - Translation\n",
        "    - Code generation\n",
        "    \n",
        "    ## Popular Models\n",
        "    - GPT-4 (OpenAI)\n",
        "    - Claude (Anthropic)\n",
        "    - PaLM (Google)\n",
        "    - LLaMA (Meta)\n",
        "    \"\"\",\n",
        "    \n",
        "    \"\"\"\n",
        "    # Embedding Models\n",
        "    \n",
        "    Embedding models convert text into dense vector representations that capture\n",
        "    semantic meaning. These vectors enable semantic search and similarity comparisons.\n",
        "    \n",
        "    ## How Embeddings Work\n",
        "    1. Text is tokenized into smaller units\n",
        "    2. Tokens are processed through neural networks\n",
        "    3. Output is a fixed-size vector (e.g., 1024 dimensions)\n",
        "    4. Similar texts have similar vectors\n",
        "    \n",
        "    ## Applications\n",
        "    - Semantic search\n",
        "    - Recommendation systems\n",
        "    - Clustering and classification\n",
        "    - RAG systems\n",
        "    \"\"\",\n",
        "    \n",
        "    \"\"\"\n",
        "    # Vector Databases\n",
        "    \n",
        "    Vector databases are specialized databases designed to store and search\n",
        "    high-dimensional vectors efficiently using similarity metrics.\n",
        "    \n",
        "    ## Popular Vector Databases\n",
        "    - Pinecone: Fully managed, cloud-native\n",
        "    - ChromaDB: Open-source, embedded\n",
        "    - Weaviate: Open-source with hybrid search\n",
        "    - Qdrant: Open-source with rich filtering\n",
        "    - FAISS: Facebook's similarity search library\n",
        "    \n",
        "    ## Key Features\n",
        "    - Fast similarity search\n",
        "    - Scalability to billions of vectors\n",
        "    - Metadata filtering\n",
        "    - Hybrid search capabilities\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "# Add documents with metadata\n",
        "metadatas = [\n",
        "    {\"title\": \"Large Language Models\", \"category\": \"AI Fundamentals\"},\n",
        "    {\"title\": \"Embedding Models\", \"category\": \"AI Fundamentals\"},\n",
        "    {\"title\": \"Vector Databases\", \"category\": \"Infrastructure\"}\n",
        "]\n",
        "\n",
        "num_chunks = rag.add_documents(\n",
        "    documents=documents,\n",
        "    metadatas=metadatas,\n",
        "    document_type=\"markdown\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Added {len(documents)} documents\")\n",
        "print(f\"üì¶ Created {num_chunks} chunks\")\n",
        "print(f\"üìä Total documents in KB: {rag.get_stats()['total_documents']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Query the RAG System\n",
        "\n",
        "Now let's ask questions and see how the system retrieves relevant context and generates answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define test queries\n",
        "queries = [\n",
        "    \"What are the main capabilities of Large Language Models?\",\n",
        "    \"How do embedding models work?\",\n",
        "    \"What are some popular vector databases and their features?\",\n",
        "    \"Explain the relationship between embeddings and RAG systems.\"\n",
        "]\n",
        "\n",
        "# Process each query\n",
        "for i, query in enumerate(queries, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Query {i}: {query}\")\n",
        "    print('='*80)\n",
        "    \n",
        "    # Get RAG response\n",
        "    response = rag.query(\n",
        "        query=query,\n",
        "        top_k=3,  # Retrieve top 3 most relevant chunks\n",
        "        temperature=0.3  # Lower temperature for more focused answers\n",
        "    )\n",
        "    \n",
        "    # Display answer\n",
        "    print(f\"\\nüìù Answer:\\n{response.answer}\")\n",
        "    \n",
        "    # Display sources\n",
        "    print(f\"\\nüìö Sources (relevance scores):\")\n",
        "    for idx, (source, score) in enumerate(response.sources, 1):\n",
        "        print(f\"  {idx}. [Score: {score:.3f}] {source[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Advanced Usage - Custom Documents\n",
        "\n",
        "You can also add your own documents. Let's add a document from a file or URL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add a custom document about your specific domain\n",
        "custom_document = \"\"\"\n",
        "# Your Custom Knowledge Base\n",
        "\n",
        "Add your own domain-specific content here. This could be:\n",
        "- Company documentation\n",
        "- Technical specifications\n",
        "- Research papers\n",
        "- Product information\n",
        "- FAQ content\n",
        "\n",
        "The RAG system will chunk this content and make it searchable.\n",
        "\"\"\"\n",
        "\n",
        "# Add the custom document\n",
        "rag.add_document(\n",
        "    document=custom_document,\n",
        "    metadata={\"title\": \"Custom Content\", \"category\": \"User Data\"},\n",
        "    document_type=\"markdown\"\n",
        ")\n",
        "\n",
        "# Query about the custom content\n",
        "custom_query = \"What kind of content can I add to my custom knowledge base?\"\n",
        "response = rag.query(custom_query, top_k=2)\n",
        "\n",
        "print(f\"Query: {custom_query}\")\n",
        "print(f\"\\nAnswer: {response.answer}\")\n",
        "print(f\"\\nüìä Updated stats: {rag.get_stats()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Performance Analysis\n",
        "\n",
        "Let's analyze the performance of our RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Test retrieval speed\n",
        "test_queries = [\n",
        "    \"What is machine learning?\",\n",
        "    \"How does RAG work?\",\n",
        "    \"What are embeddings?\",\n",
        "    \"Explain vector databases\",\n",
        "    \"What is Claude?\"\n",
        "]\n",
        "\n",
        "print(\"‚è±Ô∏è Performance Testing\\n\")\n",
        "times = []\n",
        "\n",
        "for query in test_queries:\n",
        "    start = time.time()\n",
        "    response = rag.query(query, top_k=3)\n",
        "    elapsed = time.time() - start\n",
        "    times.append(elapsed)\n",
        "    print(f\"Query: '{query[:30]}...' - Time: {elapsed:.2f}s\")\n",
        "\n",
        "avg_time = np.mean(times)\n",
        "print(f\"\\nüìä Average query time: {avg_time:.2f}s\")\n",
        "print(f\"üìä Min/Max: {min(times):.2f}s / {max(times):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you have a working RAG system, you can:\n",
        "\n",
        "1. **Add more documents**: Load PDFs, web pages, or databases\n",
        "2. **Optimize chunking**: Experiment with different chunk sizes and overlap\n",
        "3. **Try different models**: Use Claude 3 Opus for higher quality or Sonnet for balance\n",
        "4. **Add metadata filtering**: Filter search results by category, date, etc.\n",
        "5. **Build an API**: Wrap this in a FastAPI or Flask service\n",
        "6. **Create a UI**: Build a chat interface with Gradio or Streamlit\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Anthropic Documentation](https://docs.anthropic.com/)\n",
        "- [Voyage AI Documentation](https://docs.voyageai.com/)\n",
        "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
        "- [RAG Best Practices](https://github.com/anthropics/anthropic-cookbook)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
